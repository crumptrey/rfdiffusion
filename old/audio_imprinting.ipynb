{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4159c8f1-ea74-4788-8a1f-ac87d106e600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/home/trey/experiment_rfdiffusion/models/saved_models/impainting' already exists.\n",
      "Checkpoint loaded. Model trained for 49 epochs. Last recorded loss: 0.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 0 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 1 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 2 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 3 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 0 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 1 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 2 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inpainting (noise=0.00): 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveform 3 processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from audio_diffusion_pytorch.audio_diffusion_pytorch import DiffusionModel, UNetV0, VDiffusion_Inpainted, VSampler, VInpainter, RePaintInpainter\n",
    "import utils.load_datasets\n",
    "import utils.training\n",
    "import networks.transforms as net_transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters\n",
    "train_modulations = ['AM-SSB', 'CPFSK', 'QPSK', 'GFSK', 'PAM4', 'QAM16', 'WBFM', '8PSK', 'QAM64', 'AM-DSB', 'BPSK']\n",
    "train_SNRs = np.arange(-20, 19, 2)\n",
    "test_modulations = ['OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK', '16APSK', '32APSK', '64APSK', \n",
    "                    '128APSK', '16QAM', '32QAM', '64QAM', '128QAM', '256QAM', 'AM-SSB-WC', 'AM-SSB-SC', 'AM-DSB-WC', \n",
    "                    'AM-DSB-SC', 'FM', 'GMSK', 'OQPSK']\n",
    "test_SNRs = np.arange(-20, 30, 2)\n",
    "dataset_train_name = '2016.10A'\n",
    "dataset_test_name = '2016.10A'\n",
    "dataDir = '/home/trey/experiment_rfdiffusion/models/saved_models/impainting'\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "adam_betas = (0.9, 0.999)\n",
    "model_save_dir = '/home/trey/experiment_rfdiffusion/models/saved_models/impainting'\n",
    "\n",
    "# Create directories if they do not exist\n",
    "utils.training.create_directory(dataDir)\n",
    "\n",
    "# Define data split ratios\n",
    "split = [0.75, 0.05, 0.20]\n",
    "\n",
    "# Define data transformations\n",
    "train_transforms = transforms.Compose([net_transforms.PowerNormalization()])\n",
    "test_transforms = train_transforms\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = utils.load_datasets.getDataset(\n",
    "    dataset_train_name, dataset_test_name, train_modulations, train_SNRs, test_modulations, test_SNRs, split, dataDir, train_transforms, test_transforms\n",
    ")\n",
    "\n",
    "# Create data loader\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DiffusionModel(\n",
    "    net_t=UNetV0,  # The model type used for diffusion (U-Net V0 in this case)\n",
    "    in_channels=2,  # U-Net: number of input/output (audio) channels\n",
    "    channels=[64, 128, 256, 512],  # U-Net: channels at each layer\n",
    "    factors=[2, 2, 2, 2],  # U-Net: downsampling and upsampling factors at each layer\n",
    "    items=[2, 2, 2, 2],  # U-Net: number of repeating items at each layer\n",
    "    attentions=[1, 1, 1, 1],  # U-Net: attention enabled/disabled at each layer\n",
    "    attention_heads=4,  # U-Net: number of attention heads per attention item\n",
    "    attention_features=32,  # U-Net: number of attention features per attention item\n",
    "    diffusion_t=VDiffusion_Inpainted,  # The diffusion method used\n",
    "    use_text_conditioning=False,  # U-Net: enables text conditioning (default T5-base)\n",
    "    use_embedding_cfg=False,  # U-Net: enables classifier free guidance\n",
    ")\n",
    "\n",
    "# Define the path to the checkpoint file\n",
    "checkpoint_path = os.path.join(model_save_dir, 'model_epoch_49.pth')\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, betas=adam_betas)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Optionally, load the epoch and loss\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print('Checkpoint loaded. Model trained for {} epochs. Last recorded loss: {:.4f}'.format(epoch, loss))\n",
    "net = UNetV0(\n",
    "    dim = 1,\n",
    "    in_channels=2,  # U-Net: number of input/output (audio) channels\n",
    "    channels=[64, 128, 256, 512],  # U-Net: channels at each layer\n",
    "    factors=[2, 2, 2, 2],  # U-Net: downsampling and upsampling factors at each layer\n",
    "    items=[2, 2, 2, 2],  # U-Net: number of repeating items at each layer\n",
    "    attentions=[1, 1, 1, 1],  # U-Net: attention enabled/disabled at each layer\n",
    "    attention_heads=4,  # U-Net: number of attention heads per attention item\n",
    "    attention_features=32,  # U-Net: number of attention features per attention item    \n",
    ").to(device)\n",
    "# Initialize inpainter with the trained model\n",
    "inpainter = RePaintInpainter(net=net)\n",
    "\n",
    "# Create results directory\n",
    "results_folder = Path(dataDir) / 'results'\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to plot and save waveforms\n",
    "def plot_waveforms(original, masked, generated, modulation, snr, index):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original waveform\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(original[0, 0, :], label='I')\n",
    "    plt.plot(original[0, 1, :], label='Q')\n",
    "    plt.title(f'Original Waveform - {modulation}, SNR {snr} dB')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot masked waveform\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(masked[0, 0, :], label='I')\n",
    "    plt.plot(masked[0, 1, :], label='Q')\n",
    "    plt.title(f'Masked Waveform - {modulation}, SNR {snr} dB')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot generated waveform\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(generated[0, 0, :], label='I')\n",
    "    plt.plot(generated[0, 1, :], label='Q')\n",
    "    plt.title(f'Generated Waveform - {modulation}, SNR {snr} dB')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_folder / f'waveforms_{modulation}_{snr}dB_{index}.png')\n",
    "    plt.close()\n",
    "\n",
    "def generate_random_mask(signal_length, hole_ratio, min_hole_size=16, max_hole_size=32):\n",
    "    \"\"\"\n",
    "    Generate a random binary mask with specified hole ratio for the IQ signal.\n",
    "    \n",
    "    Args:\n",
    "        signal_length (int): Length of the signal.\n",
    "        hole_ratio (float): Ratio of the signal that should be masked (between 0 and 1).\n",
    "        min_hole_size (int): Minimum size of the holes.\n",
    "        max_hole_size (int): Maximum size of the holes.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask with True for present and False for missing parts.\n",
    "    \"\"\"\n",
    "    num_samples_to_mask = int(signal_length * hole_ratio)\n",
    "    \n",
    "    # Initialize mask with all True\n",
    "    mask = torch.ones(signal_length, dtype=torch.bool)\n",
    "    \n",
    "    # Determine the maximum size of a hole\n",
    "    max_hole_size = min(max_hole_size, signal_length)  # Ensure it fits within signal length\n",
    "    \n",
    "    # Randomly place holes\n",
    "    while num_samples_to_mask > 0:\n",
    "        # Randomly choose hole size\n",
    "        hole_size = np.random.randint(min_hole_size, max_hole_size + 1)\n",
    "        \n",
    "        # Ensure hole size does not exceed remaining number of samples to mask\n",
    "        hole_size = min(hole_size, num_samples_to_mask)\n",
    "        \n",
    "        # Randomly choose a position for the hole\n",
    "        start_index = np.random.randint(0, signal_length - hole_size + 1)\n",
    "        end_index = start_index + hole_size\n",
    "        \n",
    "        # Apply the hole to the mask\n",
    "        mask[start_index:end_index] = False\n",
    "        \n",
    "        num_samples_to_mask -= hole_size\n",
    "    \n",
    "    return mask\n",
    "    \n",
    "# Imprinting process\n",
    "for i, data in enumerate(data_loader):\n",
    "    waveforms, labels, snrs = data\n",
    "    waveforms = waveforms.to(device)\n",
    "    \n",
    "    for j in range(waveforms.size(0)):\n",
    "        original_waveform = waveforms[j].unsqueeze(0)\n",
    "        modulation = train_modulations[labels[j].item()]\n",
    "        snr = train_SNRs[snrs[j].item()]\n",
    "\n",
    "        # Create a random mask using the new function\n",
    "        mask = generate_random_mask(original_waveform.shape[-1], hole_ratio=0.2, min_hole_size=16, max_hole_size=32)\n",
    "        mask = mask.to(device)\n",
    "        masked_waveform = original_waveform * mask\n",
    "        # Inpaint the masked waveform\n",
    "        generated_waveform = inpainter(\n",
    "            source=original_waveform,\n",
    "            mask=mask,\n",
    "            num_steps=10,  # Number of inpainting steps\n",
    "            num_resamples=20,  # Number of resampling steps\n",
    "            show_progress=True,\n",
    "        )\n",
    "\n",
    "        # Plot and save the waveforms\n",
    "        plot_waveforms(original_waveform.cpu().numpy(), masked_waveform.cpu().numpy(), generated_waveform.cpu().detach().numpy(), modulation, snr, j)\n",
    "        print(f'Waveform {j} processed and saved.')\n",
    "\n",
    "    if i == 1:  # Process only the first batch for demonstration purposes\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0300210b-b885-46c5-900a-76fcfacaecb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6ff2d-b166-4840-b723-bb0e3f066540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
