{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ed9e8a9-38f9-4cf5-8b45-7e571eaafd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum, Tensor\n",
    "from torch.nn import Module, ModuleList\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import utils.load_datasets\n",
    "import utils.training\n",
    "import utils.logging\n",
    "from networks import *\n",
    "import networks.transforms as net_transforms\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Optional, Sequence, Type, TypeVar, Union\n",
    "from utils import *\n",
    "import os\n",
    "from transformers import AutoTokenizer, T5EncoderModel\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# constants\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# helpers functions\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def has_int_squareroot(num):\n",
    "    return (math.sqrt(num) ** 2) == num\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "def convert_image_to_fn(img_type, image):\n",
    "    if image.mode != img_type:\n",
    "        return image.convert(img_type)\n",
    "    return image\n",
    "\n",
    "# normalization functions\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "# data\n",
    "\n",
    "def prob_mask_like(shape, prob, device):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
    "    else:\n",
    "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n",
    "\n",
    "class Dataset1D(Dataset):\n",
    "    def __init__(self, tensor: Tensor):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx].clone()\n",
    "\n",
    "# small helper modules\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "def Upsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv1d(dim, default(dim_out, dim), 3, padding = 1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out = None):\n",
    "    return nn.Conv1d(dim, default(dim_out, dim), 4, 2, 1)\n",
    "\n",
    "class RMSNorm(Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n",
    "\n",
    "class PreNorm(Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "class T5Embedder(nn.Module):\n",
    "    def __init__(self, model: str = \"t5-base\", max_length: int = 2):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.transformer = T5EncoderModel.from_pretrained(model)\n",
    "        self.max_length = max_length\n",
    "        self.embedding_features = self.transformer.config.d_model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, texts: Sequence[str]) -> Tensor:\n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        device = next(self.transformer.parameters()).device\n",
    "        input_ids = encoded[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "        self.transformer.eval()\n",
    "\n",
    "        embedding = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )[\"last_hidden_state\"]\n",
    "\n",
    "        return embedding\n",
    "\n",
    "class SinusoidalPosEmb(Module):\n",
    "    def __init__(self, dim, theta = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.theta) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RandomOrLearnedSinusoidalPosEmb(Module):\n",
    "    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim, is_random = False):\n",
    "        super().__init__()\n",
    "        assert (dim % 2) == 0\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered\n",
    "\n",
    "# building block modules\n",
    "\n",
    "class Block(Module):\n",
    "    def __init__(self, dim, dim_out, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = RMSNorm(dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResnetBlock(Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, text_emb_dim = None, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(int(time_emb_dim) + int(text_emb_dim), dim_out * 2)\n",
    "        ) if exists(time_emb_dim) or exists(text_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, dropout = dropout)\n",
    "        self.block2 = Block(dim_out, dim_out)\n",
    "        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None, text_emb = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb) or exists(text_emb):\n",
    "            text_emb = text_emb.squeeze()\n",
    "            cond_emb = tuple(filter(exists, (time_emb, text_emb)))\n",
    "            cond_emb = torch.cat(cond_emb, dim=-1)\n",
    "            cond_emb = self.mlp(cond_emb)\n",
    "            cond_emb = rearrange(cond_emb, 'b c -> b c 1')\n",
    "            scale_shift = cond_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "        h = self.block2(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class LinearAttention(Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, dim, 1),\n",
    "            RMSNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale        \n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c n -> b (h c) n', h = self.heads)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv1d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b (h d) n')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# model\n",
    "\n",
    "class Unet1D(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        text_embedder=None,\n",
    "        cond_drop_prob = 0.5,\n",
    "        init_dim = None,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels = 3,\n",
    "        dropout = 0.,\n",
    "        self_condition = False,\n",
    "        learned_variance = False,\n",
    "        learned_sinusoidal_cond = False,\n",
    "        random_fourier_features = False,\n",
    "        learned_sinusoidal_dim = 16,\n",
    "        sinusoidal_pos_emb_theta = 10000,\n",
    "        attn_dim_head = 32,\n",
    "        attn_heads = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Text embedder\n",
    "        self.text_embedder = text_embedder if text_embedder else T5Embedder()\n",
    "        text_emb_dim = self.text_embedder.embedding_features\n",
    "        self.null_text_emb = nn.Parameter(torch.randn(text_emb_dim))\n",
    "        # determine dimensions\n",
    "        self.cond_drop_prob = cond_drop_prob\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv1d(input_channels, init_dim, 7, padding = 3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # time embeddings\n",
    "\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n",
    "\n",
    "        if self.random_or_learned_sinusoidal_cond:\n",
    "            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n",
    "            fourier_dim = learned_sinusoidal_dim + 1\n",
    "        else:\n",
    "            sinu_pos_emb = SinusoidalPosEmb(dim, theta = sinusoidal_pos_emb_theta)\n",
    "            fourier_dim = dim\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "        resnet_block = partial(ResnetBlock, time_emb_dim = time_dim, text_emb_dim = text_emb_dim, dropout = dropout)\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.downs = ModuleList([])\n",
    "        self.ups = ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(ModuleList([\n",
    "                resnet_block(dim_in, dim_in),\n",
    "                resnet_block(dim_in, dim_in),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv1d(dim_in, dim_out, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = resnet_block(mid_dim, mid_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim, dim_head = attn_dim_head, heads = attn_heads)))\n",
    "        self.mid_block2 = resnet_block(mid_dim, mid_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(ModuleList([\n",
    "                resnet_block(dim_out + dim_in, dim_out),\n",
    "                resnet_block(dim_out + dim_in, dim_out),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else  nn.Conv1d(dim_out, dim_in, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "\n",
    "        self.final_res_block = resnet_block(init_dim * 2, init_dim)\n",
    "        self.final_conv = nn.Conv1d(init_dim, self.out_dim, 1)\n",
    "\n",
    "    def forward_with_cond_scale(\n",
    "        self,\n",
    "        *args,\n",
    "        cond_scale = 1.,\n",
    "        rescaled_phi = 0.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        logits = self.forward(*args, cond_drop_prob = 0., **kwargs)\n",
    "\n",
    "        if cond_scale == 1:\n",
    "            return logits\n",
    "\n",
    "        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n",
    "        scaled_logits = null_logits + (logits - null_logits) * cond_scale\n",
    "\n",
    "        if rescaled_phi == 0.:\n",
    "            return scaled_logits\n",
    "\n",
    "        std_fn = partial(torch.std, dim = tuple(range(1, scaled_logits.ndim)), keepdim = True)\n",
    "        rescaled_logits = scaled_logits * (std_fn(logits) / std_fn(scaled_logits))\n",
    "\n",
    "        return rescaled_logits * rescaled_phi + scaled_logits * (1. - rescaled_phi)\n",
    "\n",
    "    def forward(self, x, time, text, cond_drop_prob = None):\n",
    "        batch,device = x.shape[0], x.device\n",
    "        cond_drop_prob = default(cond_drop_prob, self.cond_drop_prob)\n",
    "        text_emb = self.text_embedder(text)\n",
    "\n",
    "        if cond_drop_prob > 0:\n",
    "            keep_mask = prob_mask_like((x.shape[0],), 1 - cond_drop_prob, device = device)\n",
    "            null_text_emb = repeat(self.null_text_emb, 'd -> b d', b = x.shape[0])\n",
    "            text_emb = torch.where(\n",
    "                rearrange(keep_mask, 'b -> b 1'),\n",
    "                text_emb,\n",
    "                null_text_emb\n",
    "            )\n",
    "         \n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t, text_emb)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t, text_emb)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t, text_emb)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t, text_emb)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t, text_emb)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block2(x, t, text_emb)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "\n",
    "        x = self.final_res_block(x, t, text_emb)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# gaussian diffusion trainer class\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)     \n",
    "\n",
    "class GaussianDiffusion1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        seq_length,\n",
    "        timesteps=1000,\n",
    "        sampling_timesteps=None,\n",
    "        objective='pred_noise',\n",
    "        beta_schedule='cosine',\n",
    "        ddim_sampling_eta=0.,\n",
    "        auto_normalize=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, \\\n",
    "            'objective must be either pred_noise (predict noise), pred_x0 (predict x0), or pred_v (predict v)'\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        self.sampling_timesteps = sampling_timesteps or timesteps\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        def register_buffer(name, val):\n",
    "            self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            loss_weight = torch.ones_like(snr)\n",
    "        elif objective == 'pred_x0':\n",
    "            loss_weight = snr\n",
    "        elif objective == 'pred_v':\n",
    "            loss_weight = snr / (snr + 1)\n",
    "\n",
    "        register_buffer('loss_weight', loss_weight)\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, text=None, cond_scale=6., clip_x_start=False, rederive_pred_noise=False):\n",
    "        model_output = self.model.forward_with_cond_scale(x, t, text, cond_scale=cond_scale)\n",
    "        maybe_clip = partial(torch.clamp, min=-1., max=1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, text, cond_scale, clip_denoised=True):\n",
    "        preds = self.model_predictions(x, t, text, cond_scale)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_start, x_t=x, t=t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, text, cond_scale=6., clip_denoised=True):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((b,), t, device=device, dtype=torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x=x, t=batched_times, text=text, cond_scale=cond_scale, clip_denoised=clip_denoised)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0.  # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, text, shape, cond_scale=6., mask=None):\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        if mask is not None:\n",
    "            img = img * mask + mask\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in reversed(range(0, self.num_timesteps)):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, text, cond_scale)\n",
    "\n",
    "            if mask is not None:\n",
    "                img = img * mask + mask\n",
    "\n",
    "        #img = self.unnormalize(img)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, text, shape, cond_scale=6., clip_denoised=True, mask=None):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        if mask is not None:\n",
    "            img = img * mask + mask\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc='sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, text, cond_scale=cond_scale, clip_x_start=clip_denoised)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "            if mask is not None:\n",
    "                img = img * mask + mask\n",
    "\n",
    "        #img = self.unnormalize(img)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, text, cond_scale=6., batch_size=16, mask=None):\n",
    "        seq_length, channels = self.seq_length, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn(text, (batch_size, channels, seq_length), cond_scale, mask=mask)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, text, t=None, lam=0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = t or self.num_timesteps - 1\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device=device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc='interpolation sample time step', total=t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, text)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = noise or torch.randn_like(x_start)\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, *, text, noise=None):\n",
    "        b, c, n = x_start.shape\n",
    "        noise = noise or torch.randn_like(x_start)\n",
    "\n",
    "        x = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "\n",
    "        model_out = self.model(x, t, text)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'Unknown objective {self.objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction='none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, n, device = *img.shape, img.device\n",
    "        assert n == self.seq_length, f'seq length must be {self.seq_length}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)\n",
    "    @torch.no_grad()\n",
    "    def repaint(self, text, shape, cond_scale=6., mask=None):\n",
    "        \"\"\"\n",
    "        Repaint (sample) using the GaussianDiffusion1D model with masking support.\n",
    "        \"\"\"\n",
    "        # Use p_sample_loop or any other method that supports masking\n",
    "        return self.p_sample_loop(text, shape, cond_scale, mask=mask)\n",
    "        \n",
    "class Trainer1D(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model: GaussianDiffusion1D,\n",
    "        dataset: Dataset,\n",
    "        *,\n",
    "        train_batch_size = 64,\n",
    "        gradient_accumulate_every = 1,\n",
    "        train_lr = 1e-4,\n",
    "        train_num_steps = 100000,\n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.995,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        save_and_sample_every = 10000,\n",
    "        num_samples = 64,\n",
    "        results_folder = './results_prompt',\n",
    "        amp = False,\n",
    "        mixed_precision_type = 'fp16',\n",
    "        split_batches = True,\n",
    "        max_grad_norm = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # accelerator\n",
    "\n",
    "        self.accelerator = Accelerator(\n",
    "            split_batches = split_batches,\n",
    "            mixed_precision = mixed_precision_type if amp else 'no'\n",
    "        )\n",
    "\n",
    "        # model\n",
    "\n",
    "        self.model = diffusion_model\n",
    "        self.channels = diffusion_model.channels\n",
    "\n",
    "        # sampling and training hyperparameters\n",
    "\n",
    "        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n",
    "        self.num_samples = num_samples\n",
    "        self.save_and_sample_every = save_and_sample_every\n",
    "\n",
    "        self.batch_size = train_batch_size\n",
    "        self.gradient_accumulate_every = gradient_accumulate_every\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.train_num_steps = train_num_steps\n",
    "\n",
    "        # dataset and dataloader\n",
    "\n",
    "        dl = DataLoader(dataset, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count(), drop_last=True)\n",
    "\n",
    "        dl = self.accelerator.prepare(dl)\n",
    "        self.dl = cycle(dl)\n",
    "\n",
    "        # optimizer\n",
    "\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n",
    "\n",
    "        # for logging results in a folder periodically\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n",
    "            self.ema.to(self.device)\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(exist_ok = True)\n",
    "\n",
    "        # step counter state\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        # prepare model, dataloader, optimizer with accelerator\n",
    "\n",
    "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    def save(self, milestone):\n",
    "        if not self.accelerator.is_local_main_process:\n",
    "            return\n",
    "\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None\n",
    "        }\n",
    "\n",
    "        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n",
    "\n",
    "    def load(self, milestone):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'), map_location=device)\n",
    "\n",
    "        model = self.accelerator.unwrap_model(self.model)\n",
    "        model.load_state_dict(data['model'])\n",
    "\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema.load_state_dict(data[\"ema\"])\n",
    "\n",
    "        if 'version' in data:\n",
    "            print(f\"loading from version {data['version']}\")\n",
    "\n",
    "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
    "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
    "\n",
    "    def train(self):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
    "\n",
    "            while self.step < self.train_num_steps:\n",
    "                self.model.train()\n",
    "\n",
    "                total_loss = 0.\n",
    "\n",
    "                for _ in range(self.gradient_accumulate_every):\n",
    "                    x, mod_class, snr = next(self.dl)\n",
    "                    x = x.to(device)\n",
    "                    train_modulations = ['AM-SSB', 'CPFSK', 'QPSK', 'GFSK', 'PAM4', 'QAM16', 'WBFM', '8PSK', 'QAM64',\n",
    "                                         'AM-DSB',\n",
    "                                         'BPSK']\n",
    "                    train_SNRs = np.arange(-20, 19, 2)\n",
    "                    prompts = []\n",
    "\n",
    "                    for i in range(x.size()[0]):\n",
    "                        modulation = train_modulations[mod_class[i].item()]\n",
    "                        snr_value = train_SNRs[snr[i].item()]\n",
    "                        prompt = f\"{modulation} modulated waveform at {snr_value} dB SNR\"\n",
    "                        prompts.append(prompt)\n",
    "\n",
    "                    with self.accelerator.autocast():\n",
    "                        loss = self.model(x, text=prompt)\n",
    "                        loss = loss / self.gradient_accumulate_every\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                    self.accelerator.backward(loss)\n",
    "\n",
    "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "\n",
    "                self.step += 1\n",
    "                if accelerator.is_main_process:\n",
    "                    self.ema.update()\n",
    "\n",
    "                    if self.step != 0 and self.step % self.save_and_sample_every == 0:\n",
    "                        self.ema.ema_model.eval()\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            milestone = self.step // self.save_and_sample_every\n",
    "                            batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "                            all_samples_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n, text=prompt), batches))\n",
    "\n",
    "                        all_samples = torch.cat(all_samples_list, dim = 0)\n",
    "\n",
    "                        torch.save(all_samples, str(self.results_folder / f'sample-{milestone}.png'))\n",
    "                        self.save(milestone)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        accelerator.print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0344f923-c343-4465-9ffa-fd79a092a6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79ad4893-c032-49d1-b655-85d5472b334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n",
      "Directory '/home/trey/experiment_rfdiffusion/models/test/' already exists.\n",
      "QAM16 modulated waveform at 14 dB SNR\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m generated_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples_per_combination):  \u001b[38;5;66;03m# Adjust to match batch size\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     sampled_waveform \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepaint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msignal_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    126\u001b[0m     generated_samples\u001b[38;5;241m.\u001b[39mappend(sampled_waveform)\n\u001b[1;32m    128\u001b[0m generated_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(generated_samples, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 785\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.repaint\u001b[0;34m(self, text, shape, cond_scale, mask)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03mRepaint (sample) using the GaussianDiffusion1D model with masking support.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;66;03m# Use p_sample_loop or any other method that supports masking\u001b[39;00m\n\u001b[0;32m--> 785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 664\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.p_sample_loop\u001b[0;34m(self, text, shape, cond_scale, mask)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)):\n\u001b[1;32m    663\u001b[0m     self_cond \u001b[38;5;241m=\u001b[39m x_start \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_condition \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     img, x_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m         img \u001b[38;5;241m=\u001b[39m img \u001b[38;5;241m*\u001b[39m mask \u001b[38;5;241m+\u001b[39m mask\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 646\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.p_sample\u001b[0;34m(self, x, t, text, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    644\u001b[0m b, \u001b[38;5;241m*\u001b[39m_, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    645\u001b[0m batched_times \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((b,), t, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 646\u001b[0m model_mean, _, model_log_variance, x_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x) \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m  \u001b[38;5;66;03m# no noise if t == 0\u001b[39;00m\n\u001b[1;32m    648\u001b[0m pred_img \u001b[38;5;241m=\u001b[39m model_mean \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m model_log_variance)\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;241m*\u001b[39m noise\n",
      "Cell \u001b[0;32mIn[27], line 633\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.p_mean_variance\u001b[0;34m(self, x, t, text, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_mean_variance\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, text, cond_scale, clip_denoised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 633\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     x_start \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mpred_x_start\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clip_denoised:\n",
      "Cell \u001b[0;32mIn[27], line 608\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.model_predictions\u001b[0;34m(self, x, t, text, cond_scale, clip_x_start, rederive_pred_noise)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cond_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6.\u001b[39m, clip_x_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rederive_pred_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 608\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_cond_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     maybe_clip \u001b[38;5;241m=\u001b[39m partial(torch\u001b[38;5;241m.\u001b[39mclamp, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m clip_x_start \u001b[38;5;28;01melse\u001b[39;00m identity\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_noise\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[27], line 411\u001b[0m, in \u001b[0;36mUnet1D.forward_with_cond_scale\u001b[0;34m(self, cond_scale, rescaled_phi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_with_cond_scale\u001b[39m(\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    410\u001b[0m ):\n\u001b[0;32m--> 411\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_drop_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cond_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "Cell \u001b[0;32mIn[27], line 448\u001b[0m, in \u001b[0;36mUnet1D.forward\u001b[0;34m(self, x, time, text, cond_drop_prob)\u001b[0m\n\u001b[1;32m    445\u001b[0m r \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block1, block2, attn, downsample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdowns:\n\u001b[0;32m--> 448\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     h\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    451\u001b[0m     x \u001b[38;5;241m=\u001b[39m block2(x, t, text_emb)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 240\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, time_emb, text_emb)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp) \u001b[38;5;129;01mand\u001b[39;00m exists(time_emb) \u001b[38;5;129;01mor\u001b[39;00m exists(text_emb):\n\u001b[1;32m    239\u001b[0m     cond_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(exists, (time_emb, text_emb)))\n\u001b[0;32m--> 240\u001b[0m     cond_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     cond_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(cond_emb)\n\u001b[1;32m    242\u001b[0m     cond_emb \u001b[38;5;241m=\u001b[39m rearrange(cond_emb, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c -> b c 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_random_mask(signal_shape, hole_ratio, min_hole_size=16, max_hole_size=32):\n",
    "    \"\"\"\n",
    "    Generate a random binary mask with specified hole ratio for the IQ signal.\n",
    "\n",
    "    Args:\n",
    "        signal_shape (tuple): Shape of the signal (e.g., (channels, seq_length)).\n",
    "        hole_ratio (float): Ratio of the signal that should be masked (between 0 and 1).\n",
    "        min_hole_size (int): Minimum size of the holes.\n",
    "        max_hole_size (int): Maximum size of the holes.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Binary mask with 1s for present and 0s for missing parts.\n",
    "    \"\"\"\n",
    "    total_samples = np.prod(signal_shape)\n",
    "    num_samples_to_mask = int(total_samples * hole_ratio)\n",
    "    \n",
    "    # Initialize mask with all ones\n",
    "    mask = torch.ones(signal_shape, dtype=torch.float32)\n",
    "\n",
    "    # Determine the maximum size of a hole\n",
    "    max_hole_size = min(max_hole_size, signal_shape[-1])  # Ensure it fits within signal length\n",
    "\n",
    "    # Randomly place holes\n",
    "    while num_samples_to_mask > 0:\n",
    "        # Randomly choose hole size\n",
    "        hole_size = np.random.randint(min_hole_size, max_hole_size + 1)\n",
    "\n",
    "        # Ensure hole size does not exceed remaining number of samples to mask\n",
    "        hole_size = min(hole_size, num_samples_to_mask)\n",
    "\n",
    "        # Randomly choose a position for the hole\n",
    "        start_index = np.random.randint(0, signal_shape[-1] - hole_size + 1)\n",
    "        end_index = start_index + hole_size\n",
    "\n",
    "        # Apply the hole to the mask\n",
    "        mask[:, start_index:end_index] = 0\n",
    "\n",
    "        num_samples_to_mask -= hole_size\n",
    "\n",
    "    return mask\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Running')\n",
    "    \n",
    "    # Parameters\n",
    "    sequence_len = 128\n",
    "    timestep = 5\n",
    "    train_modulations = ['AM-SSB', 'CPFSK', 'QPSK', 'GFSK', 'PAM4', 'QAM16', 'WBFM', '8PSK', 'QAM64', 'AM-DSB', 'BPSK']\n",
    "    train_SNRs = np.arange(-20, 19, 2)\n",
    "    test_modulations = train_modulations\n",
    "    test_SNRs = train_SNRs\n",
    "    dataset_train_name = '2016.10A'\n",
    "    dataset_test_name = '2016.10A'\n",
    "    dataDir = '/home/trey/experiment_rfdiffusion/models/test/'\n",
    "    split = [0.75, 0.05, 0.20]\n",
    "    num_samples_per_combination = 1  # Number of samples to generate per combination\n",
    "    \n",
    "    # Create result directory\n",
    "    utils.training.create_directory(dataDir)\n",
    "    \n",
    "    # Load datasets\n",
    "    train_transforms = transforms.Compose([net_transforms.PowerNormalization()])\n",
    "    test_transforms = train_transforms\n",
    "    train_dataset, valid_dataset, test_dataset = utils.load_datasets.getDataset(\n",
    "        dataset_train_name, dataset_test_name, train_modulations, train_SNRs,\n",
    "        test_modulations, test_SNRs, split, dataDir, train_transforms, test_transforms)\n",
    "\n",
    "    # Initialize model and diffusion\n",
    "    model = Unet1D(\n",
    "        dim=64,\n",
    "        channels=2,\n",
    "        dim_mults=(1, 2, 4, 8)\n",
    "    )\n",
    "\n",
    "    diffusion = GaussianDiffusion1D(\n",
    "        model,\n",
    "        seq_length=sequence_len,\n",
    "        timesteps=timestep\n",
    "    ).cuda()\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer1D(\n",
    "        diffusion,\n",
    "        train_dataset,\n",
    "        train_num_steps=10000\n",
    "    )\n",
    "    \n",
    "    # Load the pre-trained model checkpoint if available\n",
    "    trainer.load(10)\n",
    "    \n",
    "    # Prepare data loader for real samples\n",
    "    real_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    results_folder = Path(dataDir) / 'results'\n",
    "    results_folder.mkdir(exist_ok=True, parents=True)\n",
    "    train_modulations = ['QAM16']\n",
    "    train_SNRs = np.arange(14, 19, 2)\n",
    "    # Dictionary to store distances\n",
    "    distances = {}\n",
    "    signal_shape = (2,128)\n",
    "    hole_ratio = 0.25\n",
    "    mask = generate_random_mask(signal_shape, hole_ratio)\n",
    "    # Iterate over each modulation and SNR\n",
    "    for modulation in train_modulations:\n",
    "        for snr in train_SNRs:\n",
    "            class_index = train_modulations.index(modulation)\n",
    "            class_tensor = torch.IntTensor([class_index]).cuda()\n",
    "            snr_index = np.where(train_SNRs == snr)[0].item()\n",
    "            snr_tensor = torch.IntTensor([snr_index]).cuda()\n",
    "            mask = mask.cuda()\n",
    "            cond_scale = 5\n",
    "            modulation = train_modulations[class_index]\n",
    "            snr_value = train_SNRs[snr_index]\n",
    "            prompt = f\"{modulation} modulated waveform at {snr_value} dB SNR\"\n",
    "            print(prompt)\n",
    "            \n",
    "            # Sample multiple waveforms (400) for each combination using the sample method\n",
    "            generated_samples = []\n",
    "            for _ in range(num_samples_per_combination):  # Adjust to match batch size\n",
    "                sampled_waveform = diffusion.repaint(\n",
    "                    text=prompt,\n",
    "                    cond_scale=cond_scale,\n",
    "                    shape = signal_shape,\n",
    "                    mask = mask\n",
    "                ).cpu().numpy()\n",
    "                generated_samples.append(sampled_waveform)\n",
    "            \n",
    "            generated_samples = np.concatenate(generated_samples, axis=0)\n",
    "            print(modulation)\n",
    "            \n",
    "            # Generate and save the constellation plot\n",
    "            plt.figure()\n",
    "            plt.scatter(generated_samples[:, 0, :].flatten(), generated_samples[:, 1, :].flatten(), marker='.')\n",
    "            plt.title(f'Constellation Plot - {modulation}, SNR {snr} dB')\n",
    "            plt.xlabel('I')\n",
    "            plt.ylabel('Q')\n",
    "            plt.grid(True)\n",
    "            filename = results_folder / f'constellation_{modulation}_{snr}dB.png'\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "\n",
    "            # Save the generated samples and labels\n",
    "            np.savez_compressed(os.path.join(results_folder, f'samples_{modulation}_{snr}dB.npz'), \n",
    "                                samples=generated_samples, \n",
    "                                classes=np.array([class_index] * num_samples_per_combination), \n",
    "                                snrs=np.array([snr_index] * num_samples_per_combination))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a063abd-2b73-47e1-b0f3-be1937edf831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
