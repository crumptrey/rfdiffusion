{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9b68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1u73oigv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58610fec19ce4ef6ba710b1358951522"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">902768</strong> at: <a href='https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing/runs/1u73oigv' target=\"_blank\">https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing/runs/1u73oigv</a><br/> View project at: <a href='https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing' target=\"_blank\">https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240815_162536-1u73oigv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1u73oigv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a876c301d3d74f8094ae33bc52f815b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ext/trey/experiment_diffusion/experiment_rfdiffusion/wandb/run-20240815_162933-6ayiuy2o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing/runs/6ayiuy2o' target=\"_blank\">614641</a></strong> to <a href='https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing' target=\"_blank\">https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing/runs/6ayiuy2o' target=\"_blank\">https://wandb.ai/trey-crump-yale-university/autoencoder_metric_testing/runs/6ayiuy2o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1 GPUs\n",
      "Number of parameters: 2358298\n",
      "Models will be saved in: /ext/trey/experiment_diffusion/experiment_rfdiffusion/models/autoencoder_metric_testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                                                                                                                              | 0/399400 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                                                                                                                 | 1/399400 [00:16<1784:42:34, 16.09s/it]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from audio_encoders_pytorch.audio_encoders_pytorch import AutoEncoder1d, TanhBottleneck\n",
    "import utils.load_datasets\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchaudio.transforms import Spectrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_latent_space(latent_representations, labels, idx=0, save_dir = \"plots\", save_name=\"latent_space\"):\n",
    "    \"\"\"\n",
    "    Plot the latent space of the encoded signals using different methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - latent_representations: The encoded latent representations.\n",
    "    - idx: Index for distinguishing multiple plots if needed.\n",
    "    \"\"\"\n",
    "    latent_representations = latent_representations.cpu().detach().numpy()\n",
    "        # Flatten latent representations if necessary\n",
    "    if latent_representations.ndim > 2:\n",
    "        num_samples = latent_representations.shape[0]\n",
    "        num_features = np.prod(latent_representations.shape[1:])\n",
    "        latent_representations = latent_representations.reshape(num_samples, num_features)\n",
    "        \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Create a 2D histogram (heatmap) from the latent space data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    heatmap, xedges, yedges = np.histogram2d(latent_representations[:, 0], latent_representations[:, 1], bins=50, range=[[-1, 1], [-1, 1]])\n",
    "\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(heatmap.T, cmap='viridis', cbar=True, xticklabels=50, yticklabels=50)\n",
    "\n",
    "    plt.title('Latent Space Heatmap')\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.savefig(os.path.join(save_dir, f\"{save_name}_heatmap_{idx}.jpg\"), format=\"jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    max_samples = 10000  # Adjust this based on memory constraints\n",
    "    if latent_representations.shape[0] > max_samples:\n",
    "        indices = np.random.choice(latent_representations.shape[0], max_samples, replace=False)\n",
    "        latent_representations = latent_representations[indices]\n",
    "\n",
    "    pca_result = PCA(n_components=2).fit_transform(latent_representations)\n",
    "    tsne_result = TSNE(n_components=2, random_state=42).fit_transform(latent_representations)\n",
    "    \n",
    "    # Plot configurations\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Simple plot (first two dimensions)\n",
    "    axes[0].scatter(latent_representations[:, 0], latent_representations[:, 1], c=labels, alpha=0.7, cmap='viridis')\n",
    "    axes[0].set_title(f'Latent Space - Simple (Pair {idx + 1})')\n",
    "    axes[0].set_xlabel('Latent Dimension 1')\n",
    "    axes[0].set_ylabel('Latent Dimension 2')\n",
    "    axes[0].grid(True)\n",
    "    fig.colorbar(scatter, ax=axes[0])\n",
    "    \n",
    "    # PCA plot\n",
    "    axes[1].scatter(pca_result[:, 0], pca_result[:, 1], c=labels, alpha=0.7, cmap='viridis')\n",
    "    axes[1].set_title(f'Latent Space - PCA (Pair {idx + 1})')\n",
    "    axes[1].set_xlabel('PCA Component 1')\n",
    "    axes[1].set_ylabel('PCA Component 2')\n",
    "    axes[1].grid(True)\n",
    "    fig.colorbar(scatter, ax=axes[1])\n",
    "    \n",
    "    # t-SNE plot\n",
    "    axes[2].scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, alpha=0.7, cmap='viridis')\n",
    "    axes[2].set_title(f'Latent Space - t-SNE (Pair {idx + 1})')\n",
    "    axes[2].set_xlabel('t-SNE Dimension 1')\n",
    "    axes[2].set_ylabel('t-SNE Dimension 2')\n",
    "    axes[2].grid(True)\n",
    "    fig.colorbar(scatter, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f\"{save_name}_{idx}.jpg\"), format=\"jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "def plot_waveform_and_spectrogram(input_signal, decoded_signal, idx, save_dir=\"plots\", save_name = \"waveform_spectrogram\"):\n",
    "    \"\"\"\n",
    "    Plot the waveform and spectrogram of the input and decoded signals.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Time-domain signals\n",
    "    axes[0, 0].plot(input_signal.cpu().numpy().flatten(), color='blue')\n",
    "    axes[0, 0].set_title(f'Input Signal - Pair {idx + 1}')\n",
    "    \n",
    "    axes[0, 1].plot(decoded_signal.cpu().numpy().flatten(), color='red')\n",
    "    axes[0, 1].set_title(f'Decoded Signal - Pair {idx + 1}')\n",
    "    \n",
    "    # Spectrograms\n",
    "    spectrogram_transform = Spectrogram(n_fft=1024).to(input_signal.device)\n",
    "    \n",
    "    input_spectrogram = spectrogram_transform(input_signal).log2()[0, :, :].detach().cpu().numpy()\n",
    "    decoded_spectrogram = spectrogram_transform(decoded_signal).log2()[0, :, :].detach().cpu().numpy()\n",
    "    \n",
    "    axes[1, 0].imshow(input_spectrogram, aspect='auto', origin='lower')\n",
    "    axes[1, 0].set_title(f'Input Spectrogram - Pair {idx + 1}')\n",
    "    \n",
    "    axes[1, 1].imshow(decoded_spectrogram, aspect='auto', origin='lower')\n",
    "    axes[1, 1].set_title(f'Decoded Spectrogram - Pair {idx + 1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f\"{save_name}_{idx}.jpg\"), format=\"jpg\")\n",
    "    plt.close()\n",
    "\n",
    "def compute_spectrogram_loss(input_signal, decoded_signal):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction loss based on spectrograms of the input and decoded signals.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_signal: The original input signal.\n",
    "    - decoded_signal: The signal reconstructed by the autoencoder.\n",
    "    - config: Configuration dictionary with spectrogram parameters.\n",
    "    \n",
    "    Returns:\n",
    "    - spectrogram_loss: The loss between the spectrograms of input and decoded signals.\n",
    "    \"\"\"\n",
    "    spectrogram_transform = Spectrogram(n_fft=1024).to(input_signal.device)\n",
    "    \n",
    "    input_spectrogram = spectrogram_transform(input_signal)\n",
    "    decoded_spectrogram = spectrogram_transform(decoded_signal)\n",
    "    \n",
    "    spectrogram_loss = F.mse_loss(decoded_spectrogram, input_spectrogram)\n",
    "    return spectrogram_loss\n",
    "\n",
    "def setup_dataloader(batch_size, num_workers, val_split=0.2):\n",
    "    dataset = utils.load_datasets.DeepSig2018Dataset(\n",
    "        \"/ext/trey/experiment_diffusion/experiment_rfdiffusion/dataset/GOLD_XYZ_OSC.0001_1024.hdf5\")\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                              num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def evaluate_model(model, data_loader, accelerator):\n",
    "    model.eval()\n",
    "    total_time_loss = 0.0\n",
    "    total_spectrogram_loss = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    # To collect all latent representations\n",
    "    all_latent_representations = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, labels) in enumerate(data_loader):\n",
    "            x = x.to(accelerator.device)\n",
    "            y = model.encode(x)\n",
    "            y_decoded = model.decode(y)\n",
    "            #labels = labels.to(accelerator.device)  # Assuming labels are provided\n",
    "\n",
    "            # Time-domain reconstruction loss\n",
    "            time_loss = F.mse_loss(y_decoded, x)\n",
    "            total_time_loss += time_loss.item() * x.size(0)\n",
    "            \n",
    "            # Spectrogram-based reconstruction loss\n",
    "            spectrogram_loss = compute_spectrogram_loss(x, y_decoded)\n",
    "            total_spectrogram_loss += spectrogram_loss.item() * x.size(0)\n",
    "            all_labels.append(labels)\n",
    "            num_samples += x.size(0)\n",
    "            # Collect latent representations for visualization\n",
    "            all_latent_representations.append(y.cpu())\n",
    "            # Visualize the first 5 pairs in the batch\n",
    "            if batch_idx < 3:\n",
    "                for i in range(min(len(x), 1)):\n",
    "                    plot_waveform_and_spectrogram(x[i], y_decoded[i], i)\n",
    "    \n",
    "    avg_time_loss = total_time_loss / num_samples\n",
    "    avg_spectrogram_loss = total_spectrogram_loss / num_samples\n",
    "    # Concatenate all latent representations into a single tensor\n",
    "    all_latent_representations = torch.cat(all_latent_representations, dim=0)\n",
    "    plot_latent_space(all_latent_representations, all_labels)\n",
    "    # Log the losses to wandb\n",
    "    accelerator.log({\n",
    "        \"eval_time_loss\": avg_time_loss,\n",
    "        \"eval_spectrogram_loss\": avg_spectrogram_loss\n",
    "    })\n",
    "\n",
    "    return avg_time_loss, avg_spectrogram_loss\n",
    "\n",
    "def setup_model(config):\n",
    "    return AutoEncoder1d(\n",
    "        in_channels=config['ae_in_channels'],\n",
    "        channels=config['ae_channels'],\n",
    "        multipliers=config['ae_multipliers'],\n",
    "        factors=config['ae_factors'],\n",
    "        num_blocks=config['ae_num_blocks'],\n",
    "        patch_size=config['ae_patch_size'],\n",
    "        resnet_groups=config['ae_resnet_groups'],\n",
    "        bottleneck=TanhBottleneck()  # You might want to make this configurable too\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_training(config, model):\n",
    "    optimizer = Adam(model.parameters(), lr=config['learning_rate'], betas=tuple(config['adam_betas']))\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, config['gamma'])\n",
    "    return optimizer, criterion, scheduler\n",
    "\n",
    "\n",
    "def setup_dataloader(config, val_split=0.2):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    dataset = utils.load_datasets.DeepSig2018Dataset_MOD(\n",
    "        \"/ext/trey/experiment_diffusion/experiment_rfdiffusion/dataset/GOLD_XYZ_OSC.0001_1024.hdf5\")\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                              num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def setup_accelerator(config):\n",
    "    accelerator = Accelerator(log_with=\"wandb\")\n",
    "    run_name = str(random.randint(0, 10e5))\n",
    "    accelerator.init_trackers(\n",
    "        config['project_name'],\n",
    "        config=config,\n",
    "        init_kwargs={\"wandb\": {\"name\": run_name}}\n",
    "    )\n",
    "    return accelerator, run_name\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, accelerator, config):\n",
    "    model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader, scheduler\n",
    "    )\n",
    "    num_training_steps = config['epochs'] * len(train_loader)\n",
    "    progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    model.train()\n",
    "    step = 1\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        for x, _ in train_loader:\n",
    "            y = model(x)\n",
    "            loss = criterion(y, x)\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            time_loss, freq_loss = evaluate_model(model, val_loader, accelerator)\n",
    "            accelerator.log({\"training_loss\": loss, \"learning_rate\": scheduler.get_last_lr()[0]}, step=step)\n",
    "            step += 1\n",
    "            \n",
    "           \n",
    "        if epoch % config['save_every'] == 0 and accelerator.is_main_process:\n",
    "            save_checkpoint(model, optimizer, epoch, config['model_save_dir'], f'model_epoch_{epoch}.pth')\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, save_dir, filename):\n",
    "    checkpoint_path = os.path.join(save_dir, filename)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config_path = 'config_autoencoder.json'  # Specify your JSON file path here\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct model_save_dir\n",
    "    config['model_save_dir'] = os.path.join(config['base_save_dir'], config['project_name'])\n",
    "    os.makedirs(config['model_save_dir'], exist_ok=True)\n",
    "\n",
    "    accelerator, run_name = setup_accelerator(config)\n",
    "\n",
    "    model = setup_model(config)\n",
    "    optimizer, criterion, scheduler = setup_training(config, model)\n",
    "    train_loader, val_loader = setup_dataloader(config)\n",
    "\n",
    "\n",
    "    print(f\"Training on {accelerator.num_processes} GPUs\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    print(f\"Models will be saved in: {config['model_save_dir']}\")\n",
    "\n",
    "    train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, accelerator, config)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        final_checkpoint_path = os.path.join(config['model_save_dir'], f'model_{run_name}.pth')\n",
    "        save_checkpoint(accelerator.unwrap_model(model), optimizer, config['epochs'], config['model_save_dir'],\n",
    "                        final_checkpoint_path)\n",
    "\n",
    "    print(\"Training complete and models saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
